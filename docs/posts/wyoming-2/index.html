<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>🙉 Wyoming 2 - speak up | 🔮 Merlin's Beard</title>
<meta name=keywords content><meta name=description content="Last time&mldr;
In my previous post I discussed creating a client for the Wyoming Protocol called wyoming-cli that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn&rsquo;t take very long."><meta name=author content><link rel=canonical href=https://john-pettigrew.github.io/posts/wyoming-2/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://john-pettigrew.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://john-pettigrew.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://john-pettigrew.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://john-pettigrew.github.io/apple-touch-icon.png><link rel=mask-icon href=https://john-pettigrew.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://john-pettigrew.github.io/posts/wyoming-2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="🙉 Wyoming 2 - speak up"><meta property="og:description" content="Last time&mldr;
In my previous post I discussed creating a client for the Wyoming Protocol called wyoming-cli that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn&rsquo;t take very long."><meta property="og:type" content="article"><meta property="og:url" content="https://john-pettigrew.github.io/posts/wyoming-2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-02T14:56:38-06:00"><meta property="article:modified_time" content="2024-12-02T14:56:38-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="🙉 Wyoming 2 - speak up"><meta name=twitter:description content="Last time&mldr;
In my previous post I discussed creating a client for the Wyoming Protocol called wyoming-cli that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn&rsquo;t take very long."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://john-pettigrew.github.io/posts/"},{"@type":"ListItem","position":2,"name":"🙉 Wyoming 2 - speak up","item":"https://john-pettigrew.github.io/posts/wyoming-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"🙉 Wyoming 2 - speak up","name":"🙉 Wyoming 2 - speak up","description":"Last time\u0026hellip; In my previous post I discussed creating a client for the Wyoming Protocol called wyoming-cli that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn\u0026rsquo;t take very long.\n","keywords":[],"articleBody":"Last time… In my previous post I discussed creating a client for the Wyoming Protocol called wyoming-cli that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn’t take very long.\nFortunately for me I naively thought this would be an easy task. My thinking was, I would primarily need to reverse the functionality I wrote in the previous post. So instead of collecting audio from a Wyoming server, I would just need to send it to a Wyoming server. As it turns out, there ending up being a few interesting problems I hadn’t considered.\nWAV files In that previous post I explained how I was able to write PCM audio to disk as a WAV file. I created an array with the required individual fields for the WAV file header and wrote each to the file in order. For ASR, I instead needed to read that data from a file. I mainly needed the audio rate, channels, bits per sample, and of course the actual audio data. I first assumed I could just read those specific values from the same byte offsets I used when generating a WAV file. Turns out, no. WAV files can contain different header sections depending on how they were created. So when using byte offsets, I was able to parse the WAV files created with wyoming-cli, but couldn’t parse any other WAV files.\nLuckily each header section first has a fixed length identifier and then a count of the number of bytes for the rest of the section. So, all I had to do was first parse the main WAV header, and then I could just loop through any other sub-sections looking for the data I needed. For example, the “fmt \" section contains information on how to play the audio. Finally, it loops until it sees a “data” section. This is where the actual audio is contained.\ntype WAVHeaderField struct { Value []byte RequiredValue []byte Offset int64 } fmtFields := map[string]WAVHeaderField{ \"format\": { Value: make([]byte, 2), RequiredValue: []byte{0x01, 0x00}, Offset: 0, }, \"channels\": { Value: make([]byte, 2), Offset: 2, }, \"sampleRate\": { Value: make([]byte, 4), Offset: 4, }, ... } I store each value I’m looking for in a “Value” byte array. Each field has a local section “Offset” so that they can be read from the file in any order and still referenced by name in the map afterwards. Lastly, there’s an optional “RequiredValue” byte array. If set, this is compared to the previously read “Value” and an error is returned if they don’t match. This makes it really easy to add validation. For example, having a “RequiredValue” of “{0x01, 0x00}” for the “format” field makes sure that the file contains PCM audio data.\nWhen to stop listening I could send my test audio WAV file to a Wyoming server running Whisper and actual see the text I was expecting. The only problem now was, I was seeing all of the text at once. For really short audio files, this is fine. But if you have a really long audio file, then its really nice to have timestamps for the audio events. Also, one of my requirements was to be able to read the audio data from the user’s microphone. I could have waited a set number of seconds or waited until the program was exiting to try and request a transcription, but I didn’t feel like either of these would have been the greatest UX.\nLuckily, its not too difficult to determine when an audio event is occuring. PCM audio contains number representations of the audio signal. So one can look at a small window of time, like 50ms for example, and compare the largest value read to the smallest. If the difference is above some threshold, then there was likely noise in that sample. If its below, then its likely just background noise. After implementing that, it was just a matter of keeping track of when the events happened and making sure the initial noise event was included with the transcription request being sent.\nIt was just working One strange thing I noticed was that the Wyoming Whisper server I was using would respond to several messages without issue, but would stop responding to any new requests after a transcription result. I’m still not 100% sure why this is the case, but my fix was to just create a new connection for each transcription needed. I initially envisioned having one shared connection that could be reused. But not having a shared connection also greatly simplified the case where multiple transcriptions are being requested at once since each request can just listen for its own response.\nTiming Speaking of multiple responses at once, I designed wyoming-cli initially to only request one transcription at a time. Once an audio event was detected, it would start the request for a transcription. For testing, I was using my local CPU instead of a GPU to run the Whisper server and it takes a bit more time to get a response. That made it really obvious that during this waiting time, any new audio events from the microphone would be ignored.\nSo to fix that, I updated it so that the audio event detection would run on its own goroutine as would each transcription request using a pool of workers. I could have created a new goroutine for every transcription request but one benefit of using a pool of workers is that it helps to make sure that the server doesn’t get overwhelmed by a single client with lots of requests.\nDoes it work It works! I was even able to write parts of this post using my voice. And now, I have a really conveinient way to get transcriptions for some of the projects I’m working on.\nwyoming-cli asr --input_file './hello.wav' Until next time!\n","wordCount":"1036","inLanguage":"en","datePublished":"2024-12-02T14:56:38-06:00","dateModified":"2024-12-02T14:56:38-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://john-pettigrew.github.io/posts/wyoming-2/"},"publisher":{"@type":"Organization","name":"🔮 Merlin's Beard","logo":{"@type":"ImageObject","url":"https://john-pettigrew.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://john-pettigrew.github.io/ accesskey=h title="🔮 Merlin's Beard (Alt + H)">🔮 Merlin's Beard</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">🙉 Wyoming 2 - speak up</h1><div class=post-meta><span title='2024-12-02 14:56:38 -0600 CST'>December 2, 2024</span></div></header><div class=post-content><h2 id=last-time>Last time&mldr;<a hidden class=anchor aria-hidden=true href=#last-time>#</a></h2><p>In my previous post I discussed creating a client for the Wyoming Protocol called <a href=https://github.com/john-pettigrew/wyoming-cli>wyoming-cli</a> that could request spoken audio to be generated from text. I decided to continue working on this project and add what I think is the next logical piece of functionality, ASR or automatic speech recognition. This means I should be able to provide either audio data from a microphone or from an audio file and have the speech transcribed. As a bonus since these servers are typically used for talking to smart assistants, getting a response doesn&rsquo;t take very long.</p><p>Fortunately for me I naively thought this would be an easy task. My thinking was, I would primarily need to reverse the functionality I wrote in the previous post. So instead of collecting audio from a Wyoming server, I would just need to send it to a Wyoming server. As it turns out, there ending up being a few interesting problems I hadn&rsquo;t considered.</p><h2 id=wav-files>WAV files<a hidden class=anchor aria-hidden=true href=#wav-files>#</a></h2><p>In that previous post I explained how I was able to write PCM audio to disk as a WAV file. I created an array with the required individual fields for the WAV file header and wrote each to the file in order. For ASR, I instead needed to <em>read</em> that data from a file. I mainly needed the audio rate, channels, bits per sample, and of course the actual audio data. I first assumed I could just read those specific values from the same byte offsets I used when generating a WAV file. Turns out, no. WAV files can contain different header sections depending on how they were created. So when using byte offsets, I was able to parse the WAV files created with <a href=https://github.com/john-pettigrew/wyoming-cli>wyoming-cli</a>, but couldn&rsquo;t parse any other WAV files.</p><p>Luckily each header section first has a fixed length identifier and then a count of the number of bytes for the rest of the section. So, all I had to do was first parse the main WAV header, and then I could just loop through any other sub-sections looking for the data I needed. For example, the &ldquo;fmt " section contains information on how to play the audio. Finally, it loops until it sees a &ldquo;data&rdquo; section. This is where the actual audio is contained.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>type</span> <span style=color:#a6e22e>WAVHeaderField</span> <span style=color:#66d9ef>struct</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Value</span>         []<span style=color:#66d9ef>byte</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>RequiredValue</span> []<span style=color:#66d9ef>byte</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Offset</span>        <span style=color:#66d9ef>int64</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a6e22e>fmtFields</span> <span style=color:#f92672>:=</span> <span style=color:#66d9ef>map</span>[<span style=color:#66d9ef>string</span>]<span style=color:#a6e22e>WAVHeaderField</span>{
</span></span><span style=display:flex><span>	<span style=color:#e6db74>&#34;format&#34;</span>: {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Value</span>:         make([]<span style=color:#66d9ef>byte</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>RequiredValue</span>: []<span style=color:#66d9ef>byte</span>{<span style=color:#ae81ff>0x01</span>, <span style=color:#ae81ff>0x00</span>},
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Offset</span>:        <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>	},
</span></span><span style=display:flex><span>	<span style=color:#e6db74>&#34;channels&#34;</span>: {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Value</span>:  make([]<span style=color:#66d9ef>byte</span>, <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Offset</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>	},
</span></span><span style=display:flex><span>	<span style=color:#e6db74>&#34;sampleRate&#34;</span>: {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Value</span>:  make([]<span style=color:#66d9ef>byte</span>, <span style=color:#ae81ff>4</span>),
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>Offset</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>	},
</span></span><span style=display:flex><span>	<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>I store each value I&rsquo;m looking for in a &ldquo;Value&rdquo; byte array. Each field has a local section &ldquo;Offset&rdquo; so that they can be read from the file in any order and still referenced by name in the map afterwards. Lastly, there&rsquo;s an optional &ldquo;RequiredValue&rdquo; byte array. If set, this is compared to the previously read &ldquo;Value&rdquo; and an error is returned if they don&rsquo;t match. This makes it really easy to add validation. For example, having a &ldquo;RequiredValue&rdquo; of &ldquo;{0x01, 0x00}&rdquo; for the &ldquo;format&rdquo; field makes sure that the file contains <em>PCM</em> audio data.</p><h2 id=when-to-stop-listening>When to stop listening<a hidden class=anchor aria-hidden=true href=#when-to-stop-listening>#</a></h2><p>I could send my test audio WAV file to a Wyoming server running Whisper and actual see the text I was expecting. The only problem now was, I was seeing <em>all</em> of the text at once. For really short audio files, this is fine. But if you have a really long audio file, then its really nice to have timestamps for the audio events. Also, one of my requirements was to be able to read the audio data from the user&rsquo;s microphone. I could have waited a set number of seconds or waited until the program was exiting to try and request a transcription, but I didn&rsquo;t feel like either of these would have been the greatest UX.</p><p>Luckily, its not too difficult to determine when an audio event is occuring. PCM audio contains number representations of the audio signal. So one can look at a small window of time, like 50ms for example, and compare the largest value read to the smallest. If the difference is above some threshold, then there was likely noise in that sample. If its below, then its likely just background noise. After implementing that, it was just a matter of keeping track of when the events happened and making sure the initial noise event was included with the transcription request being sent.</p><h2 id=it-was-just-working>It was just working<a hidden class=anchor aria-hidden=true href=#it-was-just-working>#</a></h2><p>One strange thing I noticed was that the Wyoming Whisper server I was using would respond to several messages without issue, but would stop responding to any new requests after a transcription result. I&rsquo;m still not 100% sure why this is the case, but my fix was to just create a new connection for each transcription needed. I initially envisioned having one shared connection that could be reused. But not having a shared connection also greatly simplified the case where multiple transcriptions are being requested at once since each request can just listen for its own response.</p><h2 id=timing>Timing<a hidden class=anchor aria-hidden=true href=#timing>#</a></h2><p>Speaking of multiple responses at once, I designed <a href=https://github.com/john-pettigrew/wyoming-cli>wyoming-cli</a> initially to only request one transcription at a time. Once an audio event was detected, it would start the request for a transcription. For testing, I was using my local CPU instead of a GPU to run the Whisper server and it takes a bit more time to get a response. That made it really obvious that during this waiting time, any new audio events from the microphone would be ignored.</p><p>So to fix that, I updated it so that the audio event detection would run on its own goroutine as would each transcription request using a pool of workers. I could have created a new goroutine for <em>every</em> transcription request but one benefit of using a pool of workers is that it helps to make sure that the server doesn&rsquo;t get overwhelmed by a single client with lots of requests.</p><h2 id=does-it-work>Does it work<a hidden class=anchor aria-hidden=true href=#does-it-work>#</a></h2><p>It works! I was even able to write parts of this post using my voice. And now, I have a really conveinient way to get transcriptions for some of the projects I&rsquo;m working on.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>wyoming-cli asr --input_file <span style=color:#e6db74>&#39;./hello.wav&#39;</span>
</span></span></code></pre></div><p>Until next time!</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://john-pettigrew.github.io/>🔮 Merlin's Beard</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>